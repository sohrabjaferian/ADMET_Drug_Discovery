{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c54ca55-c5e2-498c-8e60-0588add3248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.single_pred import ADME\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from IPython.display import display, Image\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import deepsmiles\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb4abd-1d6e-4e5a-b288-cec25a87f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ADME(name = 'Caco2_Wang')\n",
    "df = data.get_data()\n",
    "splits = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543dd82a-c0b0-4f08-90ed-a1910326936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = splits['train']\n",
    "valid = splits['valid']\n",
    "test = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fe6a9-5df7-4db3-955a-326be11aec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_name = 'Drug'\n",
    "\n",
    "# # Define the file path for the text file\n",
    "# file_path = 'Drug.txt'\n",
    "\n",
    "# # Extract the column data\n",
    "# column_data = train[column_name]\n",
    "\n",
    "# # Write column data to text file\n",
    "# column_data.to_csv(file_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab513d-fbca-4ceb-80d7-2748de83948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_smiles(df, smiles_col='Drug'):\n",
    "    df['Tokens'] = df[smiles_col].apply(lambda x: _tokenize_smiles_legacy(Chem.MolFromSmiles(str(x))))\n",
    "    return df\n",
    "\n",
    "def _tokenize_smiles_legacy(mol):\n",
    "\n",
    "    tokens = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        tokens.append(f\"A:{atom.GetSymbol()}\")\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        begin, end, bond_type = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx(), bond.GetBondType()\n",
    "        tokens.append(f\"B:{begin}-{end}-{bond_type}\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenize_smiles(train)\n",
    "test = tokenize_smiles(test)\n",
    "valid = tokenize_smiles(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists('train_images'):\n",
    "    os.makedirs('train_images')\n",
    "\n",
    "# Assuming df is your pandas DataFrame\n",
    "for i in range(len(train)):\n",
    "    smiles = train.iloc[i, 1]\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        img = Draw.MolToImage(mol)\n",
    "        img_path = f'train_images/molecule_{i}.png'  # Adjusted path\n",
    "        # Save the image\n",
    "        img.save(img_path)\n",
    "        print(f'Saved {img_path}')\n",
    "        display(Image(filename=img_path))\n",
    "    else:\n",
    "        print(f'Invalid SMILES at row {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('test_images'):\n",
    "    os.makedirs('test_images')\n",
    "\n",
    "# Assuming df is your pandas DataFrame\n",
    "for i in range(len(test)):\n",
    "    smiles = test.iloc[i, 1]\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        img = Draw.MolToImage(mol)\n",
    "        img_path = f'test_images/molecule_{i}.png'  # Adjusted path\n",
    "        # Save the image\n",
    "        img.save(img_path)\n",
    "#         print(f'Saved {img_path}')\n",
    "#         display(Image(filename=img_path))\n",
    "    else:\n",
    "        print(f'Invalid SMILES at row {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists('valid_images'):\n",
    "    os.makedirs('valid_images')\n",
    "\n",
    "for i in range(len(valid)):\n",
    "    smiles = valid.iloc[i, 1]\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        img = Draw.MolToImage(mol)\n",
    "        img_path = f'valid_images/molecule_{i}.png'  # Adjusted path\n",
    "        # Save the image\n",
    "        img.save(img_path)\n",
    "#         print(f'Saved {img_path}')\n",
    "#         display(Image(filename=img_path))\n",
    "    else:\n",
    "        print(f'Invalid SMILES at row {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4375992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(df)):\n",
    "#     smiles = df.iloc[i, 1]\n",
    "#     mol = Chem.MolFromSmiles(smiles)\n",
    "#     if mol is not None:\n",
    "#         img = Draw.MolToImage(mol)\n",
    "#         img_path = f'molecule_{i}.png'\n",
    "#         ## if you want to save the images, uncomment the following two lines\n",
    "#         img.save(img_path)\n",
    "# #         print(f'Saved {img_path}')\n",
    "# #         display(Image(filename=img_path))\n",
    "#     else:\n",
    "#         print(f'Invalid SMILES at row {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = 'molecule_0.png'  # Change this to the path of the image you want to load\n",
    "# image = Image.open(image_path)\n",
    "# image.show()  # This will open the image using the default image viewer on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(data_dir):\n",
    "    # Initialize lists to store channel-wise means and standard deviations\n",
    "    channel_means = [0, 0, 0]\n",
    "    channel_stds = [0, 0, 0]\n",
    "\n",
    "    # Count the total number of images in your dataset\n",
    "    total_images = 0\n",
    "\n",
    "    # Iterate through the dataset\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.png'):  # Assuming your images are in PNG format\n",
    "            img_path = os.path.join(data_dir, filename)\n",
    "            image = np.array(Image.open(img_path))  # Convert image to numpy array\n",
    "\n",
    "            # Normalize pixel values to [0, 1]\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Calculate per-channel means and standard deviations\n",
    "            channel_means += np.mean(image, axis=(0, 1))\n",
    "            channel_stds += np.std(image, axis=(0, 1))\n",
    "\n",
    "            total_images += 1\n",
    "\n",
    "    # Calculate the overall means and standard deviations\n",
    "    overall_means = channel_means / total_images\n",
    "    overall_stds = channel_stds / total_images\n",
    "    \n",
    "    return overall_means, overall_stds\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load image paths\n",
    "        self.image_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Example labels array (replace this with your actual labels)\n",
    "train_labels = train['Y']\n",
    "test_labels = test['Y']\n",
    "valid_labels = valid['Y']\n",
    "\n",
    "# Calculate mean and std for each dataset\n",
    "train_means, train_stds = calculate_mean_std('train_images')\n",
    "test_means, test_stds = calculate_mean_std('test_images')\n",
    "valid_means, valid_stds = calculate_mean_std('valid_images')\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "])\n",
    "\n",
    "# Define datasets and data loaders for each dataset\n",
    "datasets = {}\n",
    "data_loaders = {}\n",
    "\n",
    "datasets['train'] = CustomDataset(data_dir='train_images', labels=train_labels, transform=transform)\n",
    "datasets['test'] = CustomDataset(data_dir='test_images', labels=test_labels, transform=transform)\n",
    "datasets['valid'] = CustomDataset(data_dir='valid_images', labels=valid_labels, transform=transform)\n",
    "\n",
    "for key, dataset in datasets.items():\n",
    "    batch_size = 32\n",
    "    data_loaders[key] = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if key == 'train':\n",
    "        overall_means, overall_stds = train_means, train_stds\n",
    "    elif key == 'test':\n",
    "        overall_means, overall_stds = test_means, test_stds\n",
    "    elif key == 'valid':\n",
    "        overall_means, overall_stds = valid_means, valid_stds\n",
    "\n",
    "    # Normalize images using calculated means and stds\n",
    "    transform_with_normalization = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=overall_means, std=overall_stds)\n",
    "    ])\n",
    "\n",
    "    datasets[key].transform = transform_with_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_loaders['train']\n",
    "test_loader = data_loaders['test']\n",
    "val_loader = data_loaders['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa11e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error loss for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.float()  # Convert to float if the model output is double\n",
    "        loss = criterion(outputs, y.view(-1, 1).float())  # Reshape y to match output shape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Train Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images_val, y_val in val_loader:\n",
    "            outputs_val = model(images_val)\n",
    "            outputs_val = outputs_val.float()\n",
    "            val_loss += criterion(outputs_val, y_val.view(-1, 1).float()).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss:.4f}')\n",
    "\n",
    "# Test phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images_test, y_test in test_loader:\n",
    "        outputs_test = model(images_test)\n",
    "        outputs_test = outputs_test.float()\n",
    "        test_loss += criterion(outputs_test, y_test.view(-1, 1).float()).item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e92947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming you have train_loader, val_loader, and test_loader for each dataset\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error loss for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.float()  # Convert to float if the model output is double\n",
    "        loss = criterion(outputs, y.view(-1, 1).float())  # Reshape y to match output shape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Train Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images_val, y_val in val_loader:\n",
    "            outputs_val = model(images_val)\n",
    "            outputs_val = outputs_val.float()\n",
    "            val_loss += criterion(outputs_val, y_val.view(-1, 1).float()).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss:.4f}')\n",
    "\n",
    "# Test phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images_test, y_test in test_loader:\n",
    "        outputs_test = model(images_test)\n",
    "        outputs_test = outputs_test.float()\n",
    "        test_loss += criterion(outputs_test, y_test.view(-1, 1).float()).item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d55232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1  # This attribute denotes the increase in channels\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.stride != 1 or identity.shape[1] != out.shape[1]:\n",
    "            identity = F.avg_pool2d(identity, kernel_size=1, stride=self.stride)\n",
    "            identity = torch.cat((identity, torch.zeros(identity.shape[0], out.shape[1] - identity.shape[1], identity.shape[2], identity.shape[3], device=identity.device)), dim=1)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define the ResNet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1)  # Output is a single value for prediction\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the ResNet model\n",
    "model = ResNet(ResidualBlock, [2, 2, 2, 2])\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error loss for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop (assuming data_loader is defined)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, y in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, y.view(-1, 1).float())  # Reshape y to match output shape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1f6f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Assuming you have train_loader, val_loader, and test_loader for each dataset\n",
    "\n",
    "class ResNetRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetRegression, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        # Modify the last fully connected layer for regression\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = ResNetRegression()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error loss for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, y.view(-1, 1).float())  # Reshape y to match output shape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Train Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images_val, y_val in val_loader:\n",
    "            outputs_val = model(images_val)\n",
    "            val_loss += criterion(outputs_val, y_val.view(-1, 1).float()).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss:.4f}')\n",
    "\n",
    "# Test phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images_test, y_test in test_loader:\n",
    "        outputs_test = model(images_test)\n",
    "        test_loss += criterion(outputs_test, y_test.view(-1, 1).float()).item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985a6c6-ed02-41b0-9896-2d809c16bfb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze parameters of the pre-trained layers\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the number of input features for the fully connected layer\n",
    "num_ftrs = resnet.fc.in_features\n",
    "\n",
    "# Define additional layers\n",
    "additional_layers = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),  # Add a linear layer with 512 output features\n",
    "    nn.ReLU(inplace=True),     # Add ReLU activation function\n",
    "    nn.Dropout(0.2),            # Add dropout layer with 50% dropout probability\n",
    "    nn.Linear(512, 1)           # Final linear layer for single output\n",
    ")\n",
    "\n",
    "# Replace the fully connected layer in ResNet with additional_layers\n",
    "resnet.fc = additional_layers\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming train_loader, val_loader, and test_loader are defined\n",
    "# and other necessary variables are defined\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "    for images, y in train_loader:\n",
    "        images, y = images.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, y.view(-1, 1).float())  # Reshape y to match output shape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Train Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    resnet.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images_val, y_val in val_loader:\n",
    "            images_val, y_val = images_val.to(device), y_val.to(device)\n",
    "            outputs_val = resnet(images_val)\n",
    "            val_loss += criterion(outputs_val, y_val.view(-1, 1).float()).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss:.4f}')\n",
    "\n",
    "# Test phase\n",
    "resnet.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images_test, y_test in test_loader:\n",
    "        images_test, y_test = images_test.to(device), y_test.to(device)\n",
    "        outputs_test = resnet(images_test)\n",
    "        test_loss += criterion(outputs_test, y_test.view(-1, 1).float()).item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74632ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
